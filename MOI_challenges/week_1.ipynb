{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Optimization\n",
    "\n",
    "In this first introductory video of *The Math of Intelligence*, Siraj explains the foundation of optimization, and how a computer can find it with basic calculus.\n",
    "\n",
    "This is the first of several videos describing how ML algorithms are written, in order to teach the intuition needed to use higher-level tools effectively.\n",
    "\n",
    "ML is about mathmatical optimization. ML algorithms find an optimal solution by minimizing some predefined error. Optimization can be applied everywhere.\n",
    "\n",
    "Suprividsed learning is a simple type of learning that uses labeled data to determine an optimal solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression involves fitting a straight line to some data. y=mx+b is the algebraic formula describing a striaght line.\n",
    "\n",
    "This formula can also be written as:\n",
    "\n",
    "y = B0 + B1 * x\n",
    "\n",
    "This is a line where y is the output for some value of inputs x and initial estimations for the coefficients. \n",
    "\n",
    "The coefficient B0 is the y-intercept, also known as the bias. This is added to vertically offset all other points along the graph of the function representing the prediction y for input x. \n",
    "\n",
    "B1 is the line's slope coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient descent is based on using gradients to update the model parameters until a minimum is found and the gradient becomes zero. This method is used to find a global minimum on convex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using y = mx+ b\n",
    "#the error value for a set of points and a particular line can be computed by\n",
    "def compute_error(b, m, points):\n",
    "    total_error = 0\n",
    "    #print(range(len(points)))\n",
    "    for i in range(len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_error += (y - (m * x + b)) ** 2\n",
    "    return total_error / float(len(points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the ablove function were run thousands of times and error values were graphed in 3 dimensions as:\n",
    "z = error\n",
    "\n",
    "y = y-intercept (b)\n",
    "\n",
    "x = slope (m)\n",
    "\n",
    "We can see a 3d parabola with the valley of minimum values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(points, starting_b, starting_m, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, np.array(points))\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine direction of minima by finding derivative at point to know where to redraw\n",
    "def step_gradient(b_current, m_current, points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    \n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        #power rule\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x ) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current*x) + b_current))\n",
    "    new_b = b_current - b_gradient\n",
    "    new_m = m_current - m_gradient\n",
    "    \n",
    "    return [new_b, new_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate toy data\n",
    "toy_x = [i*.123*random.randint(-3, 20) for i in range(-50, 50)]\n",
    "toy_y = [i*.156*random.randint(-5, 45) for i in range(-50, 50)]\n",
    "\n",
    "\n",
    "datapoints = list(zip(toy_x, toy_y))\n",
    "datapoints = np.array(datapoints)\n",
    "#print(datapoints)\n",
    "#datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1045303563930790.0, -3.2946246550444329e+18]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner(datapoints, 2, 21, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot demonstraton\n",
    "\n",
    "#add z when working\n",
    "plot = figure()\n",
    "\n",
    "plot.circle(toy_x, toy_y)\n",
    "#plot.line()\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can a computer calculate which line has the least error?\n",
    "\n",
    "Using calculus the valley of minimum error can be determined by calculating where the derivative = 0\n",
    "\n",
    "Rather than randomly drawing lines, calculate the derivitive of some function at a given point. The derivative will point towards the minima, giving the algorithm feedback as to where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
