{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter sentiment analysis\n",
    "Create a labeled tweet dataset from api search results about any topic. Save clean tweets as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_scraper import get_tweets\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication\n",
    "Twitter requires user authentication to access its API. These keys can be found in the details of each app. https://developer.twitter.com/en/apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticate\n",
    "\"\"\"\n",
    "#create credentials file\n",
    "\n",
    "credentials={}\n",
    "credentials['CONSUMER_KEY'] = '...'\n",
    "credentials['CONSUMER_SECRET'] = '...'\n",
    "credentials['ACCESS_TOKEN'] = '...'\n",
    "credentials['ACCESS_SECRET'] = '...'\n",
    "\n",
    "with open(\"twitter_credentials.json\", \"w\") as file:  \n",
    "    json.dump(credentials, file)\n",
    "\"\"\"\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "    creds = json.load(file)\n",
    "    \n",
    "#Authorization for tweepy\n",
    "auth = tweepy.OAuthHandler(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
    "auth.set_access_token(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "my_info = api.me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search\n",
    "query = input('Enter search query: ')\n",
    "search_results = api.search(query)\n",
    "text_list = []\n",
    "polarities = []\n",
    "subjectivities = []\n",
    "\n",
    "for tweet in range(len(search_results)):\n",
    "    text_list.append(search_results[tweet].text)\n",
    "    analysis = TextBlob(search_results[tweet].text)\n",
    "    polarities.append(analysis.sentiment.polarity)\n",
    "    subjectivities.append(analysis.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lists, reshape, and save results to CSV\n",
    "text_list = pd.Series(text_list)\n",
    "polarities = pd.Series(polarities)\n",
    "subjectivities = pd.Series(subjectivities)\n",
    "\n",
    "text_df = pd.DataFrame(data=[text_list, polarities, subjectivities]).T\n",
    "text_df.columns = ['text', 'polarity', 'subjectivity']\n",
    "\n",
    "text_df.to_csv(str(query) +'_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment by user without authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_scrape(user, n_pages):\n",
    "    \"\"\"#scrape the last n pages ( < 25) of tweets from specified Twitter user, analyse sentiment and save to csv for further processing\"\"\"\n",
    "    user_tweets = []\n",
    "    \n",
    "    for tweet in get_tweets(user, pages=n_pages):\n",
    "        analysis = TextBlob(tweet['text'])\n",
    "        user_tweets.append({'text':tweet['text'], 'polarity':\"{0:.2f}\".format(round(analysis.sentiment.polarity,2)), 'subjectivity':\"{0:.2f}\".format(round(analysis.sentiment.subjectivity,2))})\n",
    "\n",
    "    tweet_df = pd.DataFrame(user_tweets)\n",
    "    tweet_df = tweet_df[['text', 'polarity', 'subjectivity']]\n",
    "    tweet_df.to_csv(str(user) +'_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_scrape('nytimes', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
